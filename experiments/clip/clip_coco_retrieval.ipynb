{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fba890ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.patches as mpatches\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "\n",
    "from transformers import CLIPProcessor\n",
    "\n",
    "sys.path.append('../..')\n",
    "from fast_nystrom_attention import CLIPModelFNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8c9575c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCODataset(Dataset):\n",
    "    def __init__(self, input_filename: str, processor: CLIPProcessor,\n",
    "                 img_key: str = \"filepath\", caption_key: str = \"captions\", sep: str = \"\\t\", token_labels_file: str = None, image_size: int = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_filename (str): Path to the CSV file.\n",
    "            processor (CLIPProcessor): Hugging Face CLIP processor for images and text.\n",
    "            img_key (str): Column name for image file paths.\n",
    "            caption_key (str): Column name for text captions.\n",
    "            sep (str): Separator used in the CSV file.\n",
    "            token_labels_file: Path to token labels file (optional).\n",
    "            image_size (int): The size to resize images to.\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(input_filename, sep=sep)\n",
    "        self.images = df[img_key].tolist()\n",
    "        self.captions = df[caption_key].tolist()\n",
    "        self.processor = processor\n",
    "        \n",
    "        if image_size:\n",
    "            self.processor.image_processor.size = {\"shortest_edge\": image_size}\n",
    "            self.processor.image_processor.crop_size = {\"height\": image_size, \"width\": image_size}\n",
    "        \n",
    "        if token_labels_file:\n",
    "            self.token_labels = torch.load(token_labels_file)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(str(self.images[idx]))\n",
    "        if image.mode != \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "        \n",
    "        captions_list = eval(self.captions[idx])\n",
    "        \n",
    "        # Process image and text\n",
    "        inputs = self.processor(text=captions_list, images=image, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        \n",
    "        image = inputs['pixel_values'].squeeze(0)\n",
    "        texts = inputs['input_ids']\n",
    "        \n",
    "        if hasattr(self, 'token_labels'):\n",
    "            token_labels = self.token_labels[idx]\n",
    "        else:\n",
    "            token_labels = None\n",
    "\n",
    "        return image, texts, token_labels\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle variable number of captions per image and\n",
    "    optional token_labels. Expects each sample to be a tuple of:\n",
    "        (image, texts, token_labels)\n",
    "    where token_labels can be None.\n",
    "    \"\"\"\n",
    "    # Unpack the batch. token_labels may be None.\n",
    "    images, texts, token_labels = zip(*batch)\n",
    "    \n",
    "    # Stack images into a batch tensor.\n",
    "    images = torch.stack(images, dim=0)  # Shape: [B, 3, H, W]\n",
    "    \n",
    "    # Process texts: pad and flatten while tracking number of captions per image.\n",
    "    num_captions = [len(t) for t in texts]\n",
    "    \n",
    "    # Find the max sequence length in the batch\n",
    "    max_len = max(t.shape[1] for t in texts)\n",
    "    \n",
    "    # Pad all text tensors to the max sequence length\n",
    "    padded_texts = [F.pad(t, (0, max_len - t.shape[1]), 'constant', 0) for t in texts]\n",
    "    \n",
    "    flattened_texts = torch.cat(padded_texts, dim=0)  # Shape: [sum(num_captions), max_seq_len]\n",
    "    \n",
    "    # Process token_labels if available (assumes similar structure as texts).\n",
    "    if token_labels[0] is not None:\n",
    "        return images, flattened_texts, num_captions, token_labels\n",
    "    else:\n",
    "        return images, flattened_texts, num_captions\n",
    "\n",
    "def get_coco_dataloader(input_filename: str, processor: CLIPProcessor, batch_size: int = 32, \n",
    "                          shuffle: bool = False, num_workers: int = 4, token_labels_file: str = None, image_size: int = None):\n",
    "    dataset = COCODataset(input_filename, processor, token_labels_file=token_labels_file, image_size=image_size)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, \n",
    "                            num_workers=num_workers, collate_fn=custom_collate_fn)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57d017c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_retrieval_metrics(image_embeddings, text_embeddings, text_gt_indices, k_values=[1, 5, 10]):\n",
    "    \"\"\"\n",
    "    Compute retrieval metrics (Recall@K) for text-to-image and image-to-text retrieval.\n",
    "    \n",
    "    Args:\n",
    "      - image_embeddings (torch.Tensor): Normalized image embeddings of shape (num_images, D)\n",
    "      - text_embeddings (torch.Tensor): Normalized text embeddings of shape (num_texts, D)\n",
    "      - text_gt_indices (list[int]): List of length num_texts where each element is the ground truth image index for that caption.\n",
    "      - k_values (list): List of K values for computing recall.\n",
    "    \n",
    "    Returns:\n",
    "      - dict: Retrieval metrics for text-to-image and image-to-text\n",
    "    \"\"\"\n",
    "    # Ensure embeddings are of compatible dimensions\n",
    "    if image_embeddings.dim() != 2 or text_embeddings.dim() != 2:\n",
    "        raise ValueError(f\"Embeddings must be 2D tensors. Got image_embeddings:{image_embeddings.shape}, text_embeddings:{text_embeddings.shape}\")\n",
    "    \n",
    "    if image_embeddings.shape[1] != text_embeddings.shape[1]:\n",
    "        raise ValueError(f\"Embedding dimensions don't match: {image_embeddings.shape[1]} vs {text_embeddings.shape[1]}\")\n",
    "        \n",
    "    # Compute similarity matrix: (num_images, num_texts)\n",
    "    similarity_matrix = image_embeddings @ text_embeddings.T\n",
    "\n",
    "    num_images = image_embeddings.shape[0]\n",
    "    num_texts = text_embeddings.shape[0]\n",
    "\n",
    "    # --- Text-to-Image Retrieval ---\n",
    "    text_to_image_metrics = {}\n",
    "    for k in k_values:\n",
    "        recalls = []\n",
    "        for text_idx in range(num_texts):\n",
    "            gt_image_idx = text_gt_indices[text_idx]\n",
    "            scores = similarity_matrix[:, text_idx]\n",
    "            top_k_indices = torch.topk(scores, k).indices\n",
    "            recalled = (gt_image_idx in top_k_indices)\n",
    "            recalls.append(recalled)\n",
    "        text_to_image_metrics[f'R@{k}'] = np.mean(recalls) * 100\n",
    "\n",
    "    # --- Image-to-Text Retrieval ---\n",
    "    image_to_text_metrics = {}\n",
    "    # Build mapping from image index to set of text indices (ground truth)\n",
    "    image_to_text_map = {i: set() for i in range(num_images)}\n",
    "    for text_idx, img_idx in enumerate(text_gt_indices):\n",
    "        image_to_text_map[img_idx].add(text_idx)\n",
    "    \n",
    "    for k in k_values:\n",
    "        recalls = []\n",
    "        for img_idx in range(num_images):\n",
    "            gt_text_indices = image_to_text_map[img_idx]\n",
    "            scores = similarity_matrix[img_idx, :]\n",
    "            top_k_texts = set(torch.topk(scores, k).indices.tolist())\n",
    "            recalled = len(gt_text_indices.intersection(top_k_texts)) > 0\n",
    "            recalls.append(recalled)\n",
    "        image_to_text_metrics[f'R@{k}'] = np.mean(recalls) * 100\n",
    "\n",
    "    return {\n",
    "        'text_to_image': text_to_image_metrics,\n",
    "        'image_to_text': image_to_text_metrics\n",
    "    }\n",
    "\n",
    "def run_retrieval_evaluation(model, val_dataloader, device):\n",
    "    \"\"\"\n",
    "    Compute retrieval metrics for the validation dataset.\n",
    "    \n",
    "    Each image may have a variable number of captions. For each image,\n",
    "    we encode the image once, and we flatten all captions across the batch,\n",
    "    recording which image each caption came from.\n",
    "    \n",
    "    Returns:\n",
    "      - dict: Retrieval metrics.\n",
    "    \"\"\"\n",
    "    image_embeddings_list = []\n",
    "    text_embeddings_list = []\n",
    "    text_gt_indices = []  # Ground-truth image index for each caption\n",
    "    global_image_counter = 0  # Keeps track of the image index across batches\n",
    "    patch_size = model.vision_model.config.patch_size\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        #for batch_idx, (images, texts, num_captions_list, token_labels) in enumerate(tqdm(val_dataloader)):\n",
    "        for batch_idx, batch in enumerate(tqdm(val_dataloader)):\n",
    "            # Handle different return formats from dataloader\n",
    "            if len(batch) == 3:\n",
    "                images, texts, num_captions_list = batch\n",
    "            else:  # len(batch) == 4\n",
    "                images, texts, num_captions_list, token_labels = batch\n",
    "                \n",
    "            # images: tensor of shape [B, 3, H, W]\n",
    "            # texts: tensor of shape [sum(num_captions), seq_length]\n",
    "            images = images.to(device)\n",
    "            # Create boolean tensor of shape B x N where the first element in each N is True\n",
    "            feature_h, feature_w = images.shape[-2] // patch_size, images.shape[-1] // patch_size\n",
    "            cls_labels = torch.zeros(images.shape[0], feature_h*feature_w+1, dtype=torch.bool, device=device)\n",
    "            cls_labels[:, 0] = True\n",
    "            zero_labels = torch.zeros_like(cls_labels, dtype=torch.bool, device=device)\n",
    "            model.load_cache({\"mask_dict\": {\"guarantee\": cls_labels, \"exclude\": zero_labels}})\n",
    "            image_emb = model.get_image_features(images, interpolate_pos_encoding=True)  # [B, D]\n",
    "            image_emb = F.normalize(image_emb, p=2, dim=-1)\n",
    "            image_embeddings_list.append(image_emb)\n",
    "            \n",
    "            for i, num_captions in enumerate(num_captions_list):\n",
    "                # Record that these captions correspond to image with global index (global_image_counter + i)\n",
    "                text_gt_indices.extend([global_image_counter + i] * num_captions)\n",
    "\n",
    "            texts = texts.to(device)\n",
    "            text_emb = model.get_text_features(input_ids=texts)  # [sum(num_captions), D]\n",
    "            if isinstance(text_emb, tuple):\n",
    "                text_emb = text_emb[0]  # Extract the embeddings if it's a tuple\n",
    "            text_emb = F.normalize(text_emb, p=2, dim=-1)\n",
    "            text_embeddings_list.append(text_emb)\n",
    "            \n",
    "            global_image_counter += images.shape[0]\n",
    "\n",
    "    # Concatenate all embeddings\n",
    "    image_embeddings = torch.cat(image_embeddings_list, dim=0)  # [num_images, D]\n",
    "    text_embeddings = torch.cat(text_embeddings_list, dim=0)    # [num_texts, D]\n",
    "    \n",
    "    assert text_embeddings.shape[0] == len(text_gt_indices), \"Number of text embeddings doesn't match number of ground truth indices\"\n",
    "    \n",
    "    metrics = compute_retrieval_metrics(image_embeddings, text_embeddings, text_gt_indices)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e5d623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"openai/clip-vit-large-patch14\"\n",
    "DTYPE = torch.bfloat16\n",
    "DEVICE = torch.device(\"cuda:0\")\n",
    "IMAGE_SIZE = 224 * 2\n",
    "fna_config = {\n",
    "    'fna_layers': range(13, 24),\n",
    "    'num_sample': 64,\n",
    "    'sampling_strategy': 'fps',\n",
    "    'sampling_features': 'q',\n",
    "    'resample_fps': False, \n",
    "}\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(MODEL_ID, use_fast=False)\n",
    "model = CLIPModelFNA.from_pretrained(\n",
    "    MODEL_ID, \n",
    "    fna_config=fna_config, \n",
    "    torch_dtype=DTYPE, \n",
    "    device_map=DEVICE\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "model.requires_grad_(False)\n",
    "\n",
    "val_dataloader = get_coco_dataloader(\n",
    "    \"/home/andrew/codebases/tmp/COCO/annotations/val.csv\", \n",
    "    processor, \n",
    "    batch_size=16, \n",
    "    shuffle=False, \n",
    "    num_workers=4,\n",
    "    image_size=IMAGE_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbd5381d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6947b7e4be841eaa80423f96fa19d4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasLtMatmulAlgoGetHeuristic( ltHandle, computeDesc.descriptor(), Adesc.descriptor(), Bdesc.descriptor(), Cdesc.descriptor(), Cdesc.descriptor(), preference.descriptor(), 1, &heuristicResult, &returnedResult)`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m retrieval_metrics = \u001b[43mrun_retrieval_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mText-to-Image Retrieval Metrics:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, value \u001b[38;5;129;01min\u001b[39;00m retrieval_metrics[\u001b[33m'\u001b[39m\u001b[33mtext_to_image\u001b[39m\u001b[33m'\u001b[39m].items():\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 96\u001b[39m, in \u001b[36mrun_retrieval_evaluation\u001b[39m\u001b[34m(model, val_dataloader, device)\u001b[39m\n\u001b[32m     94\u001b[39m zero_labels = torch.zeros_like(cls_labels, dtype=torch.bool, device=device)\n\u001b[32m     95\u001b[39m model.load_cache({\u001b[33m\"\u001b[39m\u001b[33mmask_dict\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mguarantee\u001b[39m\u001b[33m\"\u001b[39m: cls_labels, \u001b[33m\"\u001b[39m\u001b[33mexclude\u001b[39m\u001b[33m\"\u001b[39m: zero_labels}})\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m image_emb = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_image_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [B, D]\u001b[39;00m\n\u001b[32m     97\u001b[39m image_emb = F.normalize(image_emb, p=\u001b[32m2\u001b[39m, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m     98\u001b[39m image_embeddings_list.append(image_emb)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fna_dev2/lib/python3.12/site-packages/transformers/models/clip/modeling_clip.py:1291\u001b[39m, in \u001b[36mCLIPModel.get_image_features\u001b[39m\u001b[34m(self, pixel_values, output_attentions, output_hidden_states, interpolate_pos_encoding)\u001b[39m\n\u001b[32m   1286\u001b[39m output_attentions = output_attentions \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_attentions\n\u001b[32m   1287\u001b[39m output_hidden_states = (\n\u001b[32m   1288\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m   1289\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1291\u001b[39m vision_outputs: BaseModelOutputWithPooling = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvision_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1292\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1293\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1294\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1295\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1296\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1298\u001b[39m pooled_output = vision_outputs.pooler_output\n\u001b[32m   1299\u001b[39m image_features = \u001b[38;5;28mself\u001b[39m.visual_projection(pooled_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fna_dev2/lib/python3.12/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fna_dev2/lib/python3.12/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fna_dev2/lib/python3.12/site-packages/transformers/utils/generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fna_dev2/lib/python3.12/site-packages/transformers/models/clip/modeling_clip.py:1094\u001b[39m, in \u001b[36mCLIPVisionTransformer.forward\u001b[39m\u001b[34m(self, pixel_values, output_attentions, output_hidden_states, interpolate_pos_encoding)\u001b[39m\n\u001b[32m   1091\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n\u001b[32m   1092\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.pre_layrnorm(hidden_states)\n\u001b[32m-> \u001b[39m\u001b[32m1094\u001b[39m encoder_outputs: BaseModelOutput = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1096\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1097\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1098\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1100\u001b[39m last_hidden_state = encoder_outputs.last_hidden_state\n\u001b[32m   1101\u001b[39m pooled_output = last_hidden_state[:, \u001b[32m0\u001b[39m, :]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fna_dev2/lib/python3.12/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fna_dev2/lib/python3.12/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/codebases/fast_nystrom_attention/experiments/clip/../../fast_nystrom_attention/fna_models.py:426\u001b[39m, in \u001b[36mCLIPEncoderFNA.forward\u001b[39m\u001b[34m(self, inputs_embeds, attention_mask, causal_attention_mask, output_attentions, output_hidden_states)\u001b[39m\n\u001b[32m    421\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    422\u001b[39m         encoder_layer.\u001b[34m__call__\u001b[39m, \n\u001b[32m    423\u001b[39m         **layer_args,\n\u001b[32m    424\u001b[39m     )\n\u001b[32m    425\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m426\u001b[39m     layer_outputs = \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlayer_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    428\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    430\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m fna_layers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fna_dev2/lib/python3.12/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fna_dev2/lib/python3.12/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/codebases/fast_nystrom_attention/experiments/clip/../../fast_nystrom_attention/fna_models.py:311\u001b[39m, in \u001b[36mCLIPEncoderLayerFNA.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions, use_fna, sample_indices, sampling_features, sampling_strategy, num_sample, guarantee_mask, exclude_mask)\u001b[39m\n\u001b[32m    308\u001b[39m residual = hidden_states\n\u001b[32m    310\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.layer_norm1(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m hidden_states, attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_fna\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_fna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43msampling_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43msampling_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43msampling_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43msampling_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m    \u001b[49m\u001b[43mguarantee_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mguarantee_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexclude_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    326\u001b[39m residual = hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fna_dev2/lib/python3.12/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fna_dev2/lib/python3.12/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/codebases/fast_nystrom_attention/experiments/clip/../../fast_nystrom_attention/fna_models.py:192\u001b[39m, in \u001b[36mCLIPFastNystromAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions, use_fna, sample_indices, sampling_features, sampling_strategy, num_sample, guarantee_mask, exclude_mask)\u001b[39m\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_fna:\n\u001b[32m    191\u001b[39m     \u001b[38;5;28mself\u001b[39m.last_sample_indices = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample_indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m sampling_features \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mq\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mk\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mv\u001b[39m\u001b[33m\"\u001b[39m}:\n\u001b[32m    199\u001b[39m     \u001b[38;5;28mself\u001b[39m.last_sample_indices = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fna_dev2/lib/python3.12/site-packages/transformers/models/clip/modeling_clip.py:337\u001b[39m, in \u001b[36mCLIPAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001b[39m\n\u001b[32m    334\u001b[39m bsz, tgt_len, embed_dim = hidden_states.size()\n\u001b[32m    336\u001b[39m \u001b[38;5;66;03m# get query proj\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m query_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mq_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m * \u001b[38;5;28mself\u001b[39m.scale\n\u001b[32m    338\u001b[39m key_states = \u001b[38;5;28mself\u001b[39m._shape(\u001b[38;5;28mself\u001b[39m.k_proj(hidden_states), -\u001b[32m1\u001b[39m, bsz)\n\u001b[32m    339\u001b[39m value_states = \u001b[38;5;28mself\u001b[39m._shape(\u001b[38;5;28mself\u001b[39m.v_proj(hidden_states), -\u001b[32m1\u001b[39m, bsz)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fna_dev2/lib/python3.12/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fna_dev2/lib/python3.12/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fna_dev2/lib/python3.12/site-packages/torch/nn/modules/linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasLtMatmulAlgoGetHeuristic( ltHandle, computeDesc.descriptor(), Adesc.descriptor(), Bdesc.descriptor(), Cdesc.descriptor(), Cdesc.descriptor(), preference.descriptor(), 1, &heuristicResult, &returnedResult)`"
     ]
    }
   ],
   "source": [
    "retrieval_metrics = run_retrieval_evaluation(model, val_dataloader, DEVICE)\n",
    "\n",
    "print(\"Text-to-Image Retrieval Metrics:\")\n",
    "for k, value in retrieval_metrics['text_to_image'].items():\n",
    "    print(f\"{k}: {value:.2f}%\")\n",
    "\n",
    "print(\"\\nImage-to-Text Retrieval Metrics:\")\n",
    "for k, value in retrieval_metrics['image_to_text'].items():\n",
    "    print(f\"{k}: {value:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8577eaaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fna_dev2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
