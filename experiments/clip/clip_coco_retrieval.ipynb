{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fba890ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.patches as mpatches\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "\n",
    "from transformers import CLIPProcessor\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "try:\n",
    "    mp.set_start_method(\"fork\", force=True)\n",
    "except RuntimeError:\n",
    "    # Start method already set in this session\n",
    "    pass\n",
    "\n",
    "sys.path.append('../..')\n",
    "from fast_nystrom_attention import CLIPModelFNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8c9575c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCODataset(Dataset):\n",
    "    def __init__(self, input_filename: str, processor: CLIPProcessor,\n",
    "                 img_key: str = \"filepath\", caption_key: str = \"captions\", sep: str = \"\\t\", token_labels_file: str = None, image_size: int = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_filename (str): Path to the CSV file.\n",
    "            processor (CLIPProcessor): Hugging Face CLIP processor for images and text.\n",
    "            img_key (str): Column name for image file paths.\n",
    "            caption_key (str): Column name for text captions.\n",
    "            sep (str): Separator used in the CSV file.\n",
    "            token_labels_file: Path to token labels file (optional).\n",
    "            image_size (int): The size to resize images to.\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(input_filename, sep=sep)\n",
    "        self.images = df[img_key].tolist()\n",
    "        self.captions = df[caption_key].tolist()\n",
    "        self.processor = processor\n",
    "        \n",
    "        if image_size:\n",
    "            self.processor.image_processor.size = {\"shortest_edge\": image_size}\n",
    "            self.processor.image_processor.crop_size = {\"height\": image_size, \"width\": image_size}\n",
    "        \n",
    "        if token_labels_file:\n",
    "            self.token_labels = torch.load(token_labels_file)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(str(self.images[idx]))\n",
    "        if image.mode != \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "        \n",
    "        captions_list = eval(self.captions[idx])\n",
    "        \n",
    "        # Process image and text\n",
    "        inputs = self.processor(text=captions_list, images=image, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        \n",
    "        image = inputs['pixel_values'].squeeze(0)\n",
    "        texts = inputs['input_ids']\n",
    "        \n",
    "        if hasattr(self, 'token_labels'):\n",
    "            token_labels = self.token_labels[idx]\n",
    "        else:\n",
    "            token_labels = None\n",
    "\n",
    "        return image, texts, token_labels\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle variable number of captions per image and\n",
    "    optional token_labels. Expects each sample to be a tuple of:\n",
    "        (image, texts, token_labels)\n",
    "    where token_labels can be None.\n",
    "    \"\"\"\n",
    "    # Unpack the batch. token_labels may be None.\n",
    "    images, texts, token_labels = zip(*batch)\n",
    "    \n",
    "    # Stack images into a batch tensor.\n",
    "    images = torch.stack(images, dim=0)  # Shape: [B, 3, H, W]\n",
    "    \n",
    "    # Process texts: pad and flatten while tracking number of captions per image.\n",
    "    num_captions = [len(t) for t in texts]\n",
    "    \n",
    "    # Find the max sequence length in the batch\n",
    "    max_len = max(t.shape[1] for t in texts)\n",
    "    \n",
    "    # Pad all text tensors to the max sequence length\n",
    "    padded_texts = [F.pad(t, (0, max_len - t.shape[1]), 'constant', 0) for t in texts]\n",
    "    \n",
    "    flattened_texts = torch.cat(padded_texts, dim=0)  # Shape: [sum(num_captions), max_seq_len]\n",
    "    \n",
    "    # Process token_labels if available (assumes similar structure as texts).\n",
    "    if token_labels[0] is not None:\n",
    "        return images, flattened_texts, num_captions, token_labels\n",
    "    else:\n",
    "        return images, flattened_texts, num_captions\n",
    "\n",
    "def get_coco_dataloader(input_filename: str, processor: CLIPProcessor, batch_size: int = 32, \n",
    "                          shuffle: bool = False, num_workers: int = 4, token_labels_file: str = None, image_size: int = None):\n",
    "    dataset = COCODataset(input_filename, processor, token_labels_file=token_labels_file, image_size=image_size)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=custom_collate_fn,\n",
    "        multiprocessing_context=\"fork\",\n",
    "        persistent_workers=(num_workers > 0),\n",
    "        pin_memory=True\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57d017c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_retrieval_metrics(image_embeddings, text_embeddings, text_gt_indices, k_values=[1, 5, 10]):\n",
    "    \"\"\"\n",
    "    Compute retrieval metrics (Recall@K) for text-to-image and image-to-text retrieval.\n",
    "    \n",
    "    Args:\n",
    "      - image_embeddings (torch.Tensor): Normalized image embeddings of shape (num_images, D)\n",
    "      - text_embeddings (torch.Tensor): Normalized text embeddings of shape (num_texts, D)\n",
    "      - text_gt_indices (list[int]): List of length num_texts where each element is the ground truth image index for that caption.\n",
    "      - k_values (list): List of K values for computing recall.\n",
    "    \n",
    "    Returns:\n",
    "      - dict: Retrieval metrics for text-to-image and image-to-text\n",
    "    \"\"\"\n",
    "    # Ensure embeddings are of compatible dimensions\n",
    "    if image_embeddings.dim() != 2 or text_embeddings.dim() != 2:\n",
    "        raise ValueError(f\"Embeddings must be 2D tensors. Got image_embeddings:{image_embeddings.shape}, text_embeddings:{text_embeddings.shape}\")\n",
    "    \n",
    "    if image_embeddings.shape[1] != text_embeddings.shape[1]:\n",
    "        raise ValueError(f\"Embedding dimensions don't match: {image_embeddings.shape[1]} vs {text_embeddings.shape[1]}\")\n",
    "        \n",
    "    # Compute similarity matrix: (num_images, num_texts)\n",
    "    similarity_matrix = image_embeddings @ text_embeddings.T\n",
    "\n",
    "    num_images = image_embeddings.shape[0]\n",
    "    num_texts = text_embeddings.shape[0]\n",
    "\n",
    "    # --- Text-to-Image Retrieval ---\n",
    "    text_to_image_metrics = {}\n",
    "    for k in k_values:\n",
    "        recalls = []\n",
    "        for text_idx in range(num_texts):\n",
    "            gt_image_idx = text_gt_indices[text_idx]\n",
    "            scores = similarity_matrix[:, text_idx]\n",
    "            top_k_indices = torch.topk(scores, k).indices\n",
    "            recalled = (gt_image_idx in top_k_indices)\n",
    "            recalls.append(recalled)\n",
    "        text_to_image_metrics[f'R@{k}'] = np.mean(recalls) * 100\n",
    "\n",
    "    # --- Image-to-Text Retrieval ---\n",
    "    image_to_text_metrics = {}\n",
    "    # Build mapping from image index to set of text indices (ground truth)\n",
    "    image_to_text_map = {i: set() for i in range(num_images)}\n",
    "    for text_idx, img_idx in enumerate(text_gt_indices):\n",
    "        image_to_text_map[img_idx].add(text_idx)\n",
    "    \n",
    "    for k in k_values:\n",
    "        recalls = []\n",
    "        for img_idx in range(num_images):\n",
    "            gt_text_indices = image_to_text_map[img_idx]\n",
    "            scores = similarity_matrix[img_idx, :]\n",
    "            top_k_texts = set(torch.topk(scores, k).indices.tolist())\n",
    "            recalled = len(gt_text_indices.intersection(top_k_texts)) > 0\n",
    "            recalls.append(recalled)\n",
    "        image_to_text_metrics[f'R@{k}'] = np.mean(recalls) * 100\n",
    "\n",
    "    return {\n",
    "        'text_to_image': text_to_image_metrics,\n",
    "        'image_to_text': image_to_text_metrics\n",
    "    }\n",
    "\n",
    "def run_retrieval_evaluation(model, val_dataloader, device):\n",
    "    \"\"\"\n",
    "    Compute retrieval metrics for the validation dataset.\n",
    "    \n",
    "    Each image may have a variable number of captions. For each image,\n",
    "    we encode the image once, and we flatten all captions across the batch,\n",
    "    recording which image each caption came from.\n",
    "    \n",
    "    Returns:\n",
    "      - dict: Retrieval metrics.\n",
    "    \"\"\"\n",
    "    image_embeddings_list = []\n",
    "    text_embeddings_list = []\n",
    "    text_gt_indices = []  # Ground-truth image index for each caption\n",
    "    global_image_counter = 0  # Keeps track of the image index across batches\n",
    "    patch_size = model.vision_model.config.patch_size\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        #for batch_idx, (images, texts, num_captions_list, token_labels) in enumerate(tqdm(val_dataloader)):\n",
    "        for batch_idx, batch in enumerate(tqdm(val_dataloader)):\n",
    "            # Handle different return formats from dataloader\n",
    "            if len(batch) == 3:\n",
    "                images, texts, num_captions_list = batch\n",
    "            else:  # len(batch) == 4\n",
    "                images, texts, num_captions_list, token_labels = batch\n",
    "                \n",
    "            # images: tensor of shape [B, 3, H, W]\n",
    "            # texts: tensor of shape [sum(num_captions), seq_length]\n",
    "            images = images.to(device)\n",
    "            # Create boolean tensor of shape B x N where the first element in each N is True\n",
    "            feature_h, feature_w = images.shape[-2] // patch_size, images.shape[-1] // patch_size\n",
    "            cls_labels = torch.zeros(images.shape[0], feature_h*feature_w+1, dtype=torch.bool, device=device)\n",
    "            cls_labels[:, 0] = True\n",
    "            zero_labels = torch.zeros_like(cls_labels, dtype=torch.bool, device=device)\n",
    "            model.load_cache({\"mask_dict\": {\"guarantee\": cls_labels, \"exclude\": zero_labels}})\n",
    "            image_emb = model.get_image_features(images, interpolate_pos_encoding=True)  # [B, D]\n",
    "            image_emb = F.normalize(image_emb, p=2, dim=-1)\n",
    "            image_embeddings_list.append(image_emb)\n",
    "            \n",
    "            for i, num_captions in enumerate(num_captions_list):\n",
    "                # Record that these captions correspond to image with global index (global_image_counter + i)\n",
    "                text_gt_indices.extend([global_image_counter + i] * num_captions)\n",
    "\n",
    "            texts = texts.to(device)\n",
    "            text_emb = model.get_text_features(input_ids=texts)  # [sum(num_captions), D]\n",
    "            if isinstance(text_emb, tuple):\n",
    "                text_emb = text_emb[0]  # Extract the embeddings if it's a tuple\n",
    "            text_emb = F.normalize(text_emb, p=2, dim=-1)\n",
    "            text_embeddings_list.append(text_emb)\n",
    "            \n",
    "            global_image_counter += images.shape[0]\n",
    "\n",
    "    # Concatenate all embeddings\n",
    "    image_embeddings = torch.cat(image_embeddings_list, dim=0)  # [num_images, D]\n",
    "    text_embeddings = torch.cat(text_embeddings_list, dim=0)    # [num_texts, D]\n",
    "    \n",
    "    assert text_embeddings.shape[0] == len(text_gt_indices), \"Number of text embeddings doesn't match number of ground truth indices\"\n",
    "    \n",
    "    metrics = compute_retrieval_metrics(image_embeddings, text_embeddings, text_gt_indices)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e5d623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"openai/clip-vit-large-patch14\"\n",
    "DTYPE = torch.bfloat16\n",
    "DEVICE = torch.device(\"cuda:0\")\n",
    "IMAGE_SIZE = 224 * 2\n",
    "fna_config = {\n",
    "    'fna_layers': range(13, 24),\n",
    "    'num_sample': 64,\n",
    "    'sampling_strategy': 'fps',\n",
    "    'sampling_features': 'q',\n",
    "    'resample_fps': False, \n",
    "}\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(MODEL_ID, use_fast=False)\n",
    "model = CLIPModelFNA.from_pretrained(\n",
    "    MODEL_ID, \n",
    "    fna_config=fna_config, \n",
    "    torch_dtype=DTYPE, \n",
    "    device_map=DEVICE\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "model.requires_grad_(False)\n",
    "\n",
    "val_dataloader = get_coco_dataloader(\n",
    "    \"/home/andrew/codebases/tmp/COCO/annotations/val.csv\", \n",
    "    processor, \n",
    "    batch_size=16, \n",
    "    shuffle=False, \n",
    "    num_workers=4,\n",
    "    image_size=IMAGE_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbd5381d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d4488805b74a5b9a15401626619955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text-to-Image Retrieval Metrics:\n",
      "R@1: 34.06%\n",
      "R@5: 58.96%\n",
      "R@10: 69.79%\n",
      "\n",
      "Image-to-Text Retrieval Metrics:\n",
      "R@1: 52.40%\n",
      "R@5: 76.54%\n",
      "R@10: 84.88%\n"
     ]
    }
   ],
   "source": [
    "retrieval_metrics = run_retrieval_evaluation(model, val_dataloader, DEVICE)\n",
    "\n",
    "print(\"Text-to-Image Retrieval Metrics:\")\n",
    "for k, value in retrieval_metrics['text_to_image'].items():\n",
    "    print(f\"{k}: {value:.2f}%\")\n",
    "\n",
    "print(\"\\nImage-to-Text Retrieval Metrics:\")\n",
    "for k, value in retrieval_metrics['image_to_text'].items():\n",
    "    print(f\"{k}: {value:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335985d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fna_2.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
