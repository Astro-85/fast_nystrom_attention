{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7777a961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.patches as mpatches\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "\n",
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "\n",
    "sys.path.append('../..')\n",
    "from fast_nystrom_attention import LlavaNextForConditionalGenerationFNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0936aba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    \"\"\"Set random seed for reproducibility.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19586c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9175727614b941f7ba380510ab7ba095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_ID = \"llava-hf/llava-v1.6-vicuna-7b-hf\"\n",
    "DTYPE = torch.float32\n",
    "DEVICE = torch.device(\"cuda:0\")\n",
    "fna_config = {\n",
    "    'fna_layers': range(16, 32),\n",
    "    'num_sample': 512,\n",
    "    'sampling_strategy': 'fps',\n",
    "    'sampling_features': 'q',\n",
    "    'resample_fps': False, \n",
    "}\n",
    "\n",
    "processor = LlavaNextProcessor.from_pretrained(MODEL_ID, use_fast=False)\n",
    "model = LlavaNextForConditionalGenerationFNA.from_pretrained(\n",
    "    MODEL_ID, \n",
    "    fna_config=fna_config, \n",
    "    torch_dtype=DTYPE, \n",
    "    device_map=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2a879f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 500 tokens in 13.65s (36.63 tokens/s)\n",
      "USER: \n",
      "Write a long story about this image. ASSISTANT: In the heart of a mystical forest, where the trees stood tall and the air was thick with the scent of ancient secrets, there was a bridge that spanned the deepest part of the forest. It was a bridge that connected the known world to the unknown, a bridge that had stood for centuries, bearing witness to the passage of time and the whispers of the forest.\n",
      "\n",
      "The bridge was not just a physical structure; it was a bridge of the mind, a bridge of the heart, a bridge of the spirit. It was a place where the wild creatures came to drink from the stream that flowed beneath it, and where the wise old trees whispered their secrets to those who dared to listen.\n",
      "\n",
      "The forest was alive with stories, and the bridge was the heart of it all. It was a place where the spirits of the forest roamed, where the ghosts of the past haunted the present, and where the dreams of the future whispered in the wind.\n",
      "\n",
      "One day, a traveler came to the bridge. He was a man of many stories, a man who had seen the world and knew its secrets. He had traveled far and wide, but he had never been to a place like this. He had heard of the bridge, of the stories it held, and he knew that he had to cross it.\n",
      "\n",
      "As he approached the bridge, he felt a chill run down his spine. The forest was alive with whispers, and the bridge seemed to hum with a mysterious energy. He stepped onto the bridge, and as he did, he felt a weight lift from his shoulders. He was no longer a traveler; he was a seeker.\n",
      "\n",
      "The bridge was not just a bridge; it was a portal to another world. As the traveler stepped onto it, he felt the ground beneath him change. The trees grew taller, the air grew colder, and the stream flowed faster. He had entered a world of magic and mystery, a world where the veil between the known and the unknown was thin.\n",
      "\n",
      "The traveler walked along the bridge, and as he did, he felt the stories of the forest come alive around him. The trees spoke to him in hushed tones, the wind whispered secrets, and the stream sang a song that only he could hear. He was no longer alone; he\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "image = Image.open('forest.jpg')\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": \"Write a long story about this image.\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "inputs = processor(image, prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "# autoregressively complete prompt\n",
    "start_time = time.time()\n",
    "output = model.generate(**inputs, max_new_tokens=500, do_sample=False)\n",
    "end_time = time.time()\n",
    "\n",
    "generation_time = end_time - start_time\n",
    "num_generated_tokens = len(output[0]) - len(inputs.input_ids[0])\n",
    "tokens_per_second = num_generated_tokens / generation_time\n",
    "\n",
    "print(f\"Generated {num_generated_tokens} tokens in {generation_time:.2f}s ({tokens_per_second:.2f} tokens/s)\")\n",
    "\n",
    "print(processor.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a939124",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fna_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
